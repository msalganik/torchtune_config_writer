# cruijff_kit v1 - Comprehensive Summary

**Date**: 2025-11-12
**Purpose**: Reference document for understanding v1 architecture to inform v2 design
**Source**: https://github.com/niznik-dev/cruijff-kit/

---

## Executive Summary

cruijff_kit v1 is a **research toolkit for LLM fine-tuning and evaluation**, designed for Princeton researchers working with social science data on SLURM clusters. It's built for **Claude Code-assisted workflows** but supports manual operation.

**Key insight**: v1 is a **skills-driven, script-based system** where Claude Code orchestrates Python scripts via YAML configs. It's **NOT a Python package with a CLI** - it's a collection of tools, scripts, and Claude Code skills that work together.

---

## 1. REPOSITORY STRUCTURE

### Package Organization: **Loose Collection, NOT Traditional Python Package**

```
cruijff_kit/  (repository root)
‚îú‚îÄ‚îÄ __init__.py                    # Empty file (just makes it importable)
‚îú‚îÄ‚îÄ pyproject.toml                 # Minimal: only defines 2 importable subpackages
‚îÇ
‚îú‚îÄ‚îÄ tools/                         # ‚ö†Ô∏è NOT packaged - executed as scripts
‚îÇ   ‚îú‚îÄ‚îÄ torchtune/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setup_finetune.py     # Main script: generates configs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ finetune_template.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ custom_recipes/       # ‚úÖ Packaged (cruijff_kit.tools.torchtune.custom_recipes)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ lora_finetune_single_device_stable.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ custom_recipe_utils.py
‚îÇ   ‚îî‚îÄ‚îÄ inspect/
‚îÇ       ‚îú‚îÄ‚îÄ setup_inspect.py      # Main script: generates eval configs
‚îÇ       ‚îî‚îÄ‚îÄ heterogeneity/
‚îÇ
‚îú‚îÄ‚îÄ utils/                         # ‚úÖ Packaged (cruijff_kit.utils)
‚îÇ   ‚îú‚îÄ‚îÄ run_names.py
‚îÇ   ‚îú‚îÄ‚îÄ llm_utils.py
‚îÇ   ‚îú‚îÄ‚îÄ finetune_custom_metrics.py
‚îÇ   ‚îî‚îÄ‚îÄ convert_*.py
‚îÇ
‚îú‚îÄ‚îÄ experiments/                   # Research experiment types (NOT packaged)
‚îÇ   ‚îú‚îÄ‚îÄ capitalization/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cap_task.py          # Inspect-ai evaluation task
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ input/               # Dataset generation scripts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/           # setup_finetune.yaml templates
‚îÇ   ‚îî‚îÄ‚îÄ synthetic_twins/
‚îÇ
‚îú‚îÄ‚îÄ sanity_checks/                 # Testing experiments (NOT packaged)
‚îÇ
‚îú‚îÄ‚îÄ data/                          # Three-tier access control
‚îÇ   ‚îú‚îÄ‚îÄ red/     # üî¥ Sensitive (Claude Code forbidden)
‚îÇ   ‚îú‚îÄ‚îÄ yellow/  # üü° Research (Claude Code needs permission)
‚îÇ   ‚îî‚îÄ‚îÄ green/   # üü¢ Public (Claude Code allowed)
‚îÇ
‚îî‚îÄ‚îÄ .claude/skills/                # Claude Code automation
    ‚îú‚îÄ‚îÄ design-experiment/
    ‚îú‚îÄ‚îÄ scaffold-experiment/       # Orchestrator
    ‚îú‚îÄ‚îÄ scaffold-torchtune/        # Worker
    ‚îú‚îÄ‚îÄ scaffold-inspect/          # Worker
    ‚îú‚îÄ‚îÄ run-experiment/            # Orchestrator
    ‚îú‚îÄ‚îÄ run-torchtune/             # Worker
    ‚îú‚îÄ‚îÄ run-inspect/               # Worker
    ‚îî‚îÄ‚îÄ analyze-experiment/        # Planned
```

**Key Insight**: Only 2 subpackages are actually installed:
- `cruijff_kit.utils` (shared utilities)
- `cruijff_kit.tools.torchtune.custom_recipes` (modified torchtune recipes)

Everything else is **executed as scripts from the repo directory**.

---

## 2. ARCHITECTURAL PATTERN

### **NOT a CLI tool. It's a "Scripts + Skills" System**

**Traditional Tool (what v2 might become):**
```bash
cruijff-kit generate experiment.yaml  # Single command
```

**v1 Architecture (what exists now):**
```bash
# 1. User invokes Claude Code skill
# 2. Skill reads experiment_summary.md (markdown document)
# 3. Skill creates directory structure and setup_finetune.yaml files
# 4. Skill runs: python tools/torchtune/setup_finetune.py (from each run dir)
# 5. Script generates: finetune.yaml + finetune.slurm
# 6. Skill submits: sbatch finetune.slurm
```

**No single entry point. No `cruijff-kit` command. It's orchestrated workflows.**

---

## 3. CONFIGURATION ARCHITECTURE

### Two-Stage Configuration System

**Stage 1: User-Friendly Config (`setup_finetune.yaml`)**
```yaml
# High-level, researcher-friendly
experiment_name: "my_experiment"
dataset_label: "my_data"
dataset_ext: ".json"
lora_rank: 8
lr: 1e-4
epochs: 3
batch_size: 4
# ... ~25 parameters
```

**Stage 2: Torchtune Config (`finetune.yaml`)**
```yaml
# Generated by setup_finetune.py from template
# Full torchtune recipe format with all _component_ specifications
model:
  _component_: torchtune.models.llama3_2.lora_llama3_2_1b
  lora_rank: 8
  lora_alpha: 16  # Auto-calculated as 2 √ó rank
# ... ~100 lines of config
```

**Key Point**: setup_finetune.py is a **template system** that:
1. Loads `finetune_template.yaml` (torchtune base config)
2. Applies user values from `setup_finetune.yaml`
3. Adds special handling (dataset format detection, lora_alpha calculation, etc.)
4. Writes `finetune.yaml` (complete torchtune config)
5. Also writes `finetune.slurm` (SLURM script)

---

## 4. SKILLS ARCHITECTURE (Claude Code Automation)

### Hierarchical Orchestration Pattern

```
design-experiment          (Orchestrator/Planner)
    ‚Üì
scaffold-experiment        (Orchestrator)
    ‚îú‚îÄ> scaffold-torchtune (Worker: fine-tuning configs)
    ‚îî‚îÄ> scaffold-inspect   (Worker: evaluation configs)
    ‚Üì
run-experiment             (Orchestrator)
    ‚îú‚îÄ> run-torchtune      (Worker: submit + monitor fine-tuning)
    ‚îî‚îÄ> run-inspect        (Worker: submit + monitor evaluation)
    ‚Üì
analyze-experiment         (Planned)
```

**Orchestrators** coordinate, **Workers** implement.

### Workflow Example

**1. design-experiment skill**
- Claude asks researcher questions
- Creates `experiment_summary.md` (markdown table of runs)
- Documents parameters, resources, evaluation plans

**2. scaffold-experiment skill**
- Reads `experiment_summary.md`
- Calls `scaffold-torchtune`:
  - Creates run directories (e.g., `rank8_lr1e-5/`)
  - Writes `setup_finetune.yaml` for each run
  - Executes `python ../../tools/torchtune/setup_finetune.py`
  - Produces: `finetune.yaml` + `finetune.slurm`
- Calls `scaffold-inspect`:
  - Creates eval subdirectories
  - Writes `inspect.slurm` scripts

**3. run-experiment skill**
- Calls `run-torchtune`:
  - Submits all `finetune.slurm` jobs (with stagger delay)
  - Monitors SLURM queue until complete
  - Updates `experiment_summary.md` status
- Calls `run-inspect` (AFTER fine-tuning completes):
  - Verifies checkpoints exist
  - Submits evaluation jobs
  - Monitors until complete

**4. Manual alternative** (without Claude Code):
```bash
cd experiments/capitalization/
cp templates/setup_finetune_json.yaml setup_finetune.yaml
# Edit setup_finetune.yaml manually
python ../../tools/torchtune/setup_finetune.py
sbatch finetune.slurm
```

---

## 5. KEY DESIGN PATTERNS

### Pattern 1: Markdown as Configuration

**v1 uses `experiment_summary.md` as the source of truth:**

```markdown
## Fine-tuning Runs

| Run Name | Model | LoRA Rank | Learning Rate | Batch Size | Epochs | Status |
|----------|-------|-----------|---------------|------------|--------|--------|
| 1B_rank8_lr1e-4 | Llama-3.2-1B | 8 | 1e-4 | 4 | 3 | ‚è∏ Not Started |
| 1B_rank16_lr1e-4 | Llama-3.2-1B | 16 | 1e-4 | 4 | 3 | ‚è∏ Not Started |
```

Claude Code **parses this table** to generate configs.

**Problem**: Brittle. Markdown parsing is error-prone. Changes to format break automation.

---

### Pattern 2: Template-Based Config Generation

`setup_finetune.py` loads `finetune_template.yaml` and **substitutes placeholders**:

```python
# Load template
config = yaml.safe_load(open("templates/finetune_template.yaml"))

# Apply user values
config["model"]["lora_rank"] = args.lora_rank
config["model"]["lora_alpha"] = 2 * args.lora_rank  # Auto-calculate

# Handle dataset format
if args.dataset_ext == '.json':
    config["dataset"]["source"] = "json"
    config["dataset"]["data_files"] = f"{args.input_dir_base}/{args.dataset_label}.json"
elif args.dataset_ext == '.parquet':
    config["dataset"]["data_dir"] = f"{args.input_dir_base}/{args.dataset_label}/"
    # ... different structure

# Write result
with open("finetune.yaml", "w") as f:
    yaml.dump(config, f)
```

**Problem**: Template must stay in sync with torchtune versions.

---

### Pattern 3: Script-Based Orchestration

Everything is **executed as scripts from specific directories**:

```bash
# setup_finetune.py MUST be run from the run directory
cd experiments/my_exp/run_001/
python ../../tools/torchtune/setup_finetune.py
# Reads: ./setup_finetune.yaml (current dir)
# Writes: ./finetune.yaml, ./finetune.slurm (current dir)

# setup_finetune.py uses relative paths to find templates
script_dir = Path(__file__).parent  # tools/torchtune/
template_path = script_dir / "templates" / "finetune_template.yaml"
```

**Key Point**: No CLI. No `cruijff-kit` command. Just Python scripts run from specific locations.

---

## 6. SPECIAL FEATURES (What v1 Does Beyond Standard Torchtune)

### 1. Dataset Format Detection
```python
if args.dataset_ext == '.json':
    # Use instruct_dataset with data_files
elif args.dataset_ext == '.parquet':
    # Use instruct_dataset with data_dir
```

### 2. Automatic lora_alpha Calculation
```python
lora_alpha = 2 * lora_rank  # Convention
```

### 3. Multi-GPU Recipe Switching
```python
if args.gpus == 1:
    recipe = "lora_finetune_single_device"
else:
    recipe = "lora_finetune_distributed"
```

### 4. Selective Epoch Saving
```yaml
epochs_to_save: "0,2,4"  # Or "all" or "none"
save_last_epoch_only: "true"
```

### 5. Custom Torchtune Recipes
Modified recipes with:
- Validation loss tracking (nightly build)
- Custom metrics integration
- Adapter weight management
- Epoch 0-indexing

### 6. SLURM Script Generation
`setup_finetune.py` also creates `finetune.slurm`:
```bash
#!/bin/bash
#SBATCH --time=01:00:00
#SBATCH --gpus=1
#SBATCH --account=...

conda activate cruijff
tune run cruijff_kit.tools.torchtune.custom_recipes.lora_finetune_single_device_stable \
  --config finetune.yaml
```

---

## 7. EVALUATION INTEGRATION

### Inspect AI Integration
```bash
# After fine-tuning
python tools/inspect/setup_inspect.py --finetune_epoch_dir /path/to/epoch_0/
# Generates: inspect.slurm

sbatch inspect.slurm
# Runs: python experiments/capitalization/cap_task.py
# Uses: inspect-ai framework for evaluation
```

**User writes evaluation tasks** (e.g., `cap_task.py`):
```python
from inspect_ai import Task, task
from inspect_ai.dataset import json_dataset
from inspect_ai.solver import generate
from inspect_ai.scorer import match

@task
def cap_task():
    return Task(
        dataset=json_dataset("data/test.jsonl"),
        solver=[generate()],
        scorer=match()
    )
```

---

## 8. FILE STRUCTURE AFTER RUNNING v1

```
experiment_2024-11-01/
‚îú‚îÄ‚îÄ experiment_summary.md          # Markdown table of runs (created by design-experiment)
‚îú‚îÄ‚îÄ design-experiment.log
‚îú‚îÄ‚îÄ scaffold-experiment.log        # Orchestration log
‚îú‚îÄ‚îÄ scaffold-torchtune.log         # Worker log
‚îú‚îÄ‚îÄ scaffold-inspect.log           # Worker log
‚îú‚îÄ‚îÄ run-experiment.log
‚îú‚îÄ‚îÄ run-torchtune.log
‚îú‚îÄ‚îÄ run-inspect.log
‚îÇ
‚îú‚îÄ‚îÄ rank8_lr1e-5/                  # One run directory
‚îÇ   ‚îú‚îÄ‚îÄ setup_finetune.yaml        # User config (from scaffold-torchtune)
‚îÇ   ‚îú‚îÄ‚îÄ finetune.yaml              # Generated torchtune config
‚îÇ   ‚îú‚îÄ‚îÄ finetune.slurm             # Generated SLURM script
‚îÇ   ‚îú‚îÄ‚îÄ slurm-12345.out            # SLURM output
‚îÇ   ‚îî‚îÄ‚îÄ eval/                      # Evaluation subdirectory
‚îÇ       ‚îú‚îÄ‚îÄ capitalization_epoch0.slurm
‚îÇ       ‚îú‚îÄ‚îÄ slurm-12346.out
‚îÇ       ‚îî‚îÄ‚îÄ logs/
‚îÇ           ‚îî‚îÄ‚îÄ result.eval        # Inspect-ai results
‚îÇ
‚îú‚îÄ‚îÄ rank8_lr5e-5/                  # Another run
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ rank16_lr1e-5/
    ‚îî‚îÄ‚îÄ ...
```

---

## 9. PYTHON PACKAGE INSTALLATION

```bash
pip install -e .
```

**What this does:**
- Makes `cruijff_kit.utils` importable
- Makes `cruijff_kit.tools.torchtune.custom_recipes` importable
- Does NOT install any CLI commands
- Does NOT package the scripts in `tools/`

**Usage:**
```python
# Can import utilities
from cruijff_kit.utils import run_names
name = run_names.generate_model_run_name()[0]

# Can reference custom recipes
tune run cruijff_kit.tools.torchtune.custom_recipes.lora_finetune_single_device_stable \
  --config finetune.yaml
```

---

## 10. KEY QUESTIONS THIS RAISES FOR v2

### Q1: Monorepo vs. Subpackages?

**v1 is neither - it's a loose collection.**

`pyproject.toml` only packages 2 submodules:
```toml
packages = [
  "cruijff_kit.utils",
  "cruijff_kit.tools.torchtune.custom_recipes",
]
```

Everything else (tools/, experiments/, skills/) is NOT packaged.

**For v2, you need to decide:**
- **Option A**: Keep it loose (repo of scripts + skills)
- **Option B**: Create proper Python package with CLI
- **Option C**: Hybrid (package some components, leave others as scripts)

### Q2: Markdown vs. YAML?

**v1 uses markdown** (`experiment_summary.md`) as config, which Claude parses.

**Your v2 spec uses YAML** (`experiment.yaml`).

**Trade-off:**
- Markdown: Human-readable, great for documentation, Claude-friendly
- YAML: Machine-parseable, type-checkable, less brittle

**Recommendation**: YAML is right for v2. But consider: should v2 ALSO generate experiment_summary.md for humans?

### Q3: Template vs. Merge?

**v1 uses templates** (`finetune_template.yaml` with substitution).

**Your v2 spec uses merge** (load base config via `tune cp`, deep merge overrides).

**v2 approach is better**: No templates to maintain, uses torchtune's actual configs.

### Q4: Skills vs. CLI?

**v1 is skills-driven**: Claude Code orchestrates scripts.

**Your v2 spec shows CLI**: `python -m torchtune_config_writer generate experiment.yaml`

**Question**: How do skills fit in v2? Do they:
- Call the CLI? (`cruijff-kit torchtune generate experiment.yaml`)
- Import Python API? (`from cruijff_kit.torchtune import generate_configs`)
- Continue calling scripts? (`python tools/torchtune_config_writer/generate.py`)

### Q5: Component Boundaries?

**v1 has clear separation:**
- `tools/torchtune/` - Fine-tuning config generation
- `tools/inspect/` - Evaluation config generation
- Skills orchestrate them

**v2 (this repo) focuses ONLY on torchtune config generation.**

**Question**: Is this repo:
- **A component of cruijff_kit v2** (will be moved into cruijff_kit monorepo)?
- **A standalone package** that cruijff_kit v2 will use?

---

## 11. MIGRATION IMPLICATIONS

### What v2 Should Preserve from v1:

‚úÖ **Two-stage config concept** (user-friendly ‚Üí framework-specific)
‚úÖ **Special handling** (dataset formats, lora_alpha, multi-GPU)
‚úÖ **SLURM integration** (though maybe later phase)
‚úÖ **Skills orchestration** (design ‚Üí scaffold ‚Üí run ‚Üí analyze)
‚úÖ **Loose coupling** (components work independently)

### What v2 Should Change from v1:

‚ùå **Markdown parsing** ‚Üí Use YAML
‚ùå **Template system** ‚Üí Use merge strategy
‚ùå **Script-based execution** ‚Üí Proper Python package with CLI
‚ùå **Brittle paths** ‚Üí Well-defined interfaces

---

## 12. NAMING RESOLUTION

**Based on v1 evidence:**

- **Package name**: `cruijff_kit` (confirmed in pyproject.toml)
- **Repo name**: `cruijff-kit` (confirmed on GitHub)
- **Command structure**: TBD (v1 has no commands)

**For v2, the torchtune component should be:**
- **Within cruijff_kit monorepo**: `cruijff_kit/torchtune/` or `cruijff_kit/tools/torchtune_config_writer/`
- **Command**: `cruijff-kit torchtune generate experiment.yaml`
- **Python import**: `from cruijff_kit.torchtune import TorchtuneConfigGenerator`

**NOT** a separate package called `torchtune_config_writer`.

---

## 13. CONCLUSION

cruijff_kit v1 is a **research-focused, skills-driven system** for orchestrating fine-tuning and evaluation workflows on SLURM clusters. It's designed for Claude Code automation but supports manual operation.

**Key characteristics:**
- Markdown-driven (experiment_summary.md)
- Template-based config generation
- Skills orchestrate Python scripts
- Loose collection, not traditional package
- HPC-first design
- Two-tier config (user-friendly ‚Üí torchtune)

**For v2:**
- Move to YAML-based config (experiment.yaml)
- Use merge strategy instead of templates
- Create proper CLI + Python API
- Maintain component boundaries
- Integrate with existing skills architecture
- Keep special handling that makes life easier

**Biggest unanswered question**: Should this repo become a component within the cruijff_kit monorepo, or remain a separate package that cruijff_kit depends on?

---

**End of Summary**
