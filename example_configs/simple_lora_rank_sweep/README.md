# Simple LoRA Rank Sweep Example

**Phase 1 Example**: Basic torchtune config generation

This example demonstrates the core Phase 1 functionality: generating multiple torchtune configs from a single experiment definition.

## What This Example Shows

- ✅ experiment.yaml format (complex schema with metadata)
- ✅ Single variable sweep (lora_rank)
- ✅ Fixed controls (all other hyperparameters)
- ✅ Starting from torchtune recipe
- ✅ Dataset configuration
- ✅ WandB logging setup

## Files

- `experiment.yaml` - The experiment definition (what you write)
- Generated by tool:
  - `configs/run_000.yaml` through `run_003.yaml`
  - `configs/run_mapping.yaml`

## Usage

### 1. Edit experiment.yaml

Update these paths for your environment:
```yaml
dataset:
  data_files: /path/to/your/data.json  # Your dataset

output_dir: /path/to/output/lora_rank_sweep  # Where checkpoints go
```

### 2. Generate Configs

```bash
cruijff-kit torchtune generate experiment.yaml
```

Output:
```
✓ Loaded base config: llama3_2/1B_lora_single_device
✓ Generated 4 configs in configs/
  - configs/run_000.yaml (lora_rank=8)
  - configs/run_001.yaml (lora_rank=16)
  - configs/run_002.yaml (lora_rank=32)
  - configs/run_003.yaml (lora_rank=64)
✓ Created run_mapping.yaml
```

### 3. Validate (Optional)

```bash
# Validate one config
tune validate configs/run_000.yaml

# Or validate all
for config in configs/run_*.yaml; do
  tune validate $config
done
```

### 4. Run Training

**Option A: Run locally (for testing)**
```bash
tune run lora_finetune_single_device --config configs/run_000.yaml
```

**Option B: Submit to SLURM (recommended)**
```bash
# Create a simple SLURM array script
sbatch --array=0-3 submit_array.sh
```

Example `submit_array.sh`:
```bash
#!/bin/bash
#SBATCH --time=02:00:00
#SBATCH --gpus=1
#SBATCH --mem=32G

# Activate environment
conda activate your_env

# Run the config for this array task
tune run lora_finetune_single_device --config configs/run_${SLURM_ARRAY_TASK_ID}.yaml
```

## Understanding the Output

### run_mapping.yaml
Shows which parameters each run uses:
```yaml
runs:
  - id: run_000
    params:
      lora_rank: 8
  - id: run_001
    params:
      lora_rank: 16
  # ...
```

### Generated Configs
Each `run_NNN.yaml` is a complete torchtune config with:
- All parameters from the base config
- Controls applied to all runs
- Variable value for that specific run

## Next Steps

After training completes:
- Checkpoints saved to: `{output_dir}/epoch_N/`
- Logs available in WandB (offline mode by default)
- Run evaluation (Phase 2 feature - see archived examples)

## Variations

**Try different experiments by editing experiment.yaml:**

### Two-variable sweep:
```yaml
variables:
  lora_rank: [8, 16, 32]
  learning_rate: [1e-4, 3e-4]
# Generates 3 × 2 = 6 configs
```

### Custom base config:
```yaml
framework_config:
  base_config_file: /path/to/my_custom_config.yaml
```

### Different dataset format:
```yaml
controls:
  dataset:
    _component_: torchtune.datasets.instruct_dataset
    source: parquet
    data_dir: /path/to/parquet_data/
```

## See Also

- [../../SPEC.md](../../SPEC.md) - Complete technical specification
- [../../appendices/F_experiment_definition.md](../../appendices/F_experiment_definition.md) - experiment.yaml format reference
- [../../appendices/A_merge_semantics.md](../../appendices/A_merge_semantics.md) - How overrides work

## Phase 2 Preview

For examples including evaluation, see:
- [../../appendices/archive/lora_rank_with_eval/](../../appendices/archive/lora_rank_with_eval/)
