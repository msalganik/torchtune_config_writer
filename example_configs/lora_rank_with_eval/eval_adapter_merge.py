#!/usr/bin/env python3
"""
Evaluation script with LoRA adapter merging (Option A)

This is an example of evaluating LoRA adapters by:
1. Loading base model
2. Loading adapter on top
3. Merging adapter into base model
4. Evaluating merged model with Inspect AI

PROS: Simple, works with any Inspect AI setup
CONS: Uses ~15GB temp storage for merged model per eval job

Generated by tool when experiment.yaml has:
  evaluation:
    model_format: "adapter"
    merge_adapters: True  # Generate merge script
"""

import sys
import tempfile
from pathlib import Path
from inspect_ai import eval
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Add user's evaluation tasks directory to Python path
tasks_dir = Path(__file__).parent.parent / "evals"
sys.path.insert(0, str(tasks_dir))

# Import user-written tasks
from mmlu_subset import mmlu_subset
from domain_qa import domain_qa

# Configuration
BASE_MODEL = "meta-llama/Llama-3.1-8B"  # From experiment.yaml
ADAPTER_PATH = "results/run_000/adapter"  # LoRA adapter weights
RUN_ID = "run_000"

def merge_adapter_to_model(base_model_name, adapter_path):
    """
    Merge LoRA adapter into base model.

    Returns path to merged model (temporary directory).
    """
    print(f"Loading base model: {base_model_name}")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        device_map="auto",
        torch_dtype="auto"
    )

    print(f"Loading adapter: {adapter_path}")
    model = PeftModel.from_pretrained(base_model, adapter_path)

    print("Merging adapter into base model...")
    merged_model = model.merge_and_unload()

    # Save to temporary directory
    temp_dir = tempfile.mkdtemp(prefix=f"merged_model_{RUN_ID}_")
    print(f"Saving merged model to: {temp_dir}")

    merged_model.save_pretrained(temp_dir)

    # Also save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    tokenizer.save_pretrained(temp_dir)

    return temp_dir

def main():
    """Run evaluation with merged adapter."""

    # Step 1: Merge adapter into base model (temp storage)
    merged_model_path = merge_adapter_to_model(BASE_MODEL, ADAPTER_PATH)

    # Step 2: Run evaluation using merged model
    print(f"\nEvaluating merged model")
    print(f"Running {2} tasks")

    tasks = [
        mmlu_subset(subjects=["math", "physics"]),
        domain_qa(split="test", max_samples=100)
    ]

    try:
        results = eval(
            tasks,
            model=f"hf/{merged_model_path}",  # Use local merged model
            log_dir=f"eval_results/{RUN_ID}"
        )

        print(f"\n✓ Evaluation complete. Results: eval_results/{RUN_ID}")

    finally:
        # Step 3: Clean up temporary merged model
        print(f"\nCleaning up temporary model: {merged_model_path}")
        import shutil
        shutil.rmtree(merged_model_path, ignore_errors=True)

if __name__ == "__main__":
    main()


# ==============================================================================
# STORAGE NOTES
# ==============================================================================
#
# During execution:
# 1. Load base model (~15 GB in memory)
# 2. Load adapter (~20-160 MB in memory)
# 3. Merge → Save to temp dir (~15 GB disk)
# 4. Evaluate
# 5. Delete temp dir
#
# Peak storage: Base model (15 GB) + Adapter (0.02-0.16 GB) + Temp merged (15 GB)
#               ≈ 30 GB temporarily, but only adapter persists after eval
#
# On SLURM: You need ~30 GB scratch space per eval job, but final storage
#           is just the adapter weights (~MB).
#
