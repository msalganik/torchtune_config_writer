#!/usr/bin/env python3
"""
Evaluation script with direct LoRA adapter loading (Option B)

This is an example of evaluating LoRA adapters by:
1. Loading base model
2. Loading adapter on top with PEFT
3. Evaluating directly WITHOUT merging

PROS: Minimal storage (no merged model), faster
CONS: Requires custom model loading, needs Inspect AI compatibility verification

⚠️  ADVANCED: This approach requires that Inspect AI can work with PEFT models.
             This may require a custom model provider or may work out-of-box.
             VERIFY THIS WORKS in your environment before using in production.

Generated by tool when experiment.yaml has:
  evaluation:
    model_format: "adapter"
    merge_adapters: False  # Generate direct loading script
"""

import sys
from pathlib import Path
from inspect_ai import eval
from inspect_ai.model import ModelAPI, GenerateConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Add user's evaluation tasks directory to Python path
tasks_dir = Path(__file__).parent.parent / "evals"
sys.path.insert(0, str(tasks_dir))

# Import user-written tasks
from mmlu_subset import mmlu_subset
from domain_qa import domain_qa

# Configuration
BASE_MODEL = "meta-llama/Llama-3.1-8B"  # From experiment.yaml
ADAPTER_PATH = "results/run_000/adapter"  # LoRA adapter weights
RUN_ID = "run_000"


class PeftModelAPI(ModelAPI):
    """
    Custom Inspect AI ModelAPI for PEFT adapters.

    ⚠️  This is an EXAMPLE. You may need to adapt this for your needs.
        Inspect AI's ModelAPI interface may have changed.
        Check: https://inspect.aisi.org.uk/reference/inspect_ai.model.html
    """

    def __init__(self, base_model_name, adapter_path, **kwargs):
        super().__init__(**kwargs)
        self.base_model_name = base_model_name
        self.adapter_path = adapter_path
        self._model = None
        self._tokenizer = None

    def _load_model(self):
        """Lazy load model + adapter."""
        if self._model is None:
            print(f"Loading base model: {self.base_model_name}")
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                device_map="auto",
                torch_dtype="auto"
            )

            print(f"Loading adapter: {self.adapter_path}")
            self._model = PeftModel.from_pretrained(base_model, self.adapter_path)

            self._tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)

        return self._model, self._tokenizer

    async def generate(self, input: str, config: GenerateConfig):
        """Generate response using PEFT model."""
        model, tokenizer = self._load_model()

        # Tokenize input
        inputs = tokenizer(input, return_tensors="pt").to(model.device)

        # Generate
        outputs = model.generate(
            **inputs,
            max_new_tokens=config.max_tokens or 512,
            temperature=config.temperature or 0.7,
            top_p=config.top_p or 0.95,
            # Map other config parameters as needed
        )

        # Decode
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        return response


def main():
    """Run evaluation with direct adapter loading."""

    print(f"Evaluating LoRA adapter: {ADAPTER_PATH}")
    print(f"Base model: {BASE_MODEL}")
    print(f"Running {2} tasks")

    # Create custom model API with PEFT adapter
    # ⚠️  NOTE: This assumes Inspect AI accepts custom ModelAPI instances
    #           You may need to adapt this based on Inspect AI's actual interface
    model = PeftModelAPI(BASE_MODEL, ADAPTER_PATH)

    tasks = [
        mmlu_subset(subjects=["math", "physics"]),
        domain_qa(split="test", max_samples=100)
    ]

    results = eval(
        tasks,
        model=model,  # Pass custom model instance
        log_dir=f"eval_results/{RUN_ID}"
    )

    print(f"\n✓ Evaluation complete. Results: eval_results/{RUN_ID}")


if __name__ == "__main__":
    main()


# ==============================================================================
# IMPORTANT NOTES
# ==============================================================================
#
# ⚠️  This script is a TEMPLATE and may require modifications:
#
# 1. Inspect AI ModelAPI interface may have changed
#    - Check current docs: https://inspect.aisi.org.uk/models.html
#    - May need different methods (e.g., __call__ instead of generate)
#    - May need different parameter handling
#
# 2. Alternative approaches if custom ModelAPI doesn't work:
#    a) Use vLLM with LoRA adapter support (if Inspect supports vLLM)
#    b) Use Inspect's HF provider with pre-loaded PEFT model
#    c) Fall back to Option A (merge adapters)
#
# 3. Testing this approach:
#    - Run on a single eval task first
#    - Verify outputs match merged model approach
#    - Check memory usage and performance
#
# ==============================================================================
# STORAGE COMPARISON
# ==============================================================================
#
# Option A (merge): ~30 GB temp storage during eval, ~20-160 MB persistent
# Option B (direct): ~15 GB temp storage during eval, ~20-160 MB persistent
#
# Option B saves ~15 GB temporary storage per eval job!
#
